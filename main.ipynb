{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.4 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "f99de020b342f6398c2daa900581ba139c9d64acb9dee4fdd8018694687cc700"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import random\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "samples = 20000\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "df = df.loc[:samples,:]\n",
    "num_classes = len(df[\"landmark_id\"].unique())\n",
    "num_data = len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Size of training data:\", df.shape)\n",
    "print(\"Number of unique classes:\", num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = pd.DataFrame(df['landmark_id'].value_counts())\n",
    "#index the data frame\n",
    "data.reset_index(inplace=True) \n",
    "data.columns=['landmark_id','count']\n",
    "\n",
    "print(data.head(10))\n",
    "print(data.tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(data['count'].describe())#statistical data for the distribution\n",
    "plt.hist(data['count'],100,range = (0,944),label = 'test')#Histogram of the distribution\n",
    "plt.xlabel(\"Amount of images\")\n",
    "plt.ylabel(\"Occurences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Amount of classes with five and less datapoints:\", (data['count'].between(0,5)).sum()) \n",
    "\n",
    "print(\"Amount of classes with with between five and 10 datapoints:\", (data['count'].between(5,10)).sum())\n",
    "\n",
    "n = plt.hist(df[\"landmark_id\"],bins=df[\"landmark_id\"].unique())\n",
    "freq_info = n[0]\n",
    "\n",
    "plt.xlim(0,data['landmark_id'].max())\n",
    "plt.ylim(0,data['count'].max())\n",
    "plt.xlabel('Landmark ID')\n",
    "plt.ylabel('Number of images')"
   ]
  },
  {
   "source": [
    "# Training Model:\n",
    "Now, I will train the Machine Learning model for the task of landmark detection using the Python programming language which will work the same as the Google landmark detection model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "lencoder = LabelEncoder()\n",
    "lencoder.fit(df[\"landmark_id\"])\n",
    "\n",
    "def encode_label(lbl):\n",
    "    return lencoder.transform(lbl)\n",
    "    \n",
    "def decode_label(lbl):\n",
    "    return lencoder.inverse_transform(lbl)\n",
    "    \n",
    "def get_image_from_number(num):\n",
    "    fname, label = df.loc[num,:]\n",
    "    fname = fname + \".jpg\"\n",
    "    f1 = fname[0]\n",
    "    f2 = fname[1]\n",
    "    f3 = fname[2]\n",
    "    path = os.path.join(f1,f2,f3,fname)\n",
    "    im = cv2.imread(os.path.join(base_path,path))\n",
    "    return im, label\n",
    "\n",
    "print(\"4 sample images from random classes:\")\n",
    "fig=plt.figure(figsize=(16, 16))\n",
    "for i in range(1,5):\n",
    "    a = random.choices(os.listdir(base_path), k=3)\n",
    "    folder = base_path+'/'+a[0]+'/'+a[1]+'/'+a[2]\n",
    "    random_img = random.choice(os.listdir(folder))\n",
    "    img = np.array(Image.open(folder+'/'+random_img))\n",
    "    fig.add_subplot(1, 4, i)\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import VGG19\n",
    "from keras.layers import *\n",
    "from keras import Sequential\n",
    "\n",
    "### Parameters\n",
    "# learning_rate   = 0.0001\n",
    "# decay_speed     = 1e-6\n",
    "# momentum        = 0.09\n",
    "\n",
    "# loss_function   = \"sparse_categorical_crossentropy\"\n",
    "source_model = VGG19(weights=None)\n",
    "#new_layer = Dense(num_classes, activation=activations.softmax, name='prediction')\n",
    "drop_layer = Dropout(0.5)\n",
    "drop_layer2 = Dropout(0.5)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "for layer in source_model.layers[:-1]: # go through until last layer\n",
    "    if layer == source_model.layers[-25]:\n",
    "        model.add(BatchNormalization())\n",
    "    model.add(layer)\n",
    "#     if layer == source_model.layers[-3]:\n",
    "#         model.add(drop_layer)\n",
    "# model.add(drop_layer2)\n",
    "model.add(Dense(num_classes, activation=\"softmax\"))\n",
    "model.summary()\n",
    "\n",
    "\n",
    "opt1 = keras.optimizers.RMSprop(learning_rate = 0.0001, momentum = 0.09)\n",
    "opt2 = keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07)\n",
    "model.compile(optimizer=opt1,\n",
    "             loss=\"sparse_categorical_crossentropy\",\n",
    "             metrics=[\"accuracy\"])\n",
    "\n",
    "#sgd = SGD(lr=learning_rate, decay=decay_speed, momentum=momentum, nesterov=True)\n",
    "# rms = keras.optimizers.RMSprop(lr=learning_rate, momentum=momentum)\n",
    "# model.compile(optimizer=rms,\n",
    "#               loss=loss_function,\n",
    "#               metrics=[\"accuracy\"])\n",
    "# print(\"Model compiled! \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function used for processing the data, fitted into a data generator.\n",
    "def get_image_from_number(num, df):\n",
    "    fname, label = df.iloc[num,:]\n",
    "    fname = fname + \".jpg\"\n",
    "    f1 = fname[0]\n",
    "    f2 = fname[1]\n",
    "    f3 = fname[2]\n",
    "    path = os.path.join(f1,f2,f3,fname)\n",
    "    im = cv2.imread(os.path.join(base_path,path))\n",
    "    return im, label\n",
    "\n",
    "def image_reshape(im, target_size):\n",
    "    return cv2.resize(im, target_size)\n",
    "    \n",
    "def get_batch(dataframe,start, batch_size):\n",
    "    image_array = []\n",
    "    label_array = []\n",
    "    \n",
    "    end_img = start+batch_size\n",
    "    if end_img > len(dataframe):\n",
    "        end_img = len(dataframe)\n",
    "\n",
    "    for idx in range(start, end_img):\n",
    "        n = idx\n",
    "        im, label = get_image_from_number(n, dataframe)\n",
    "        im = image_reshape(im, (224, 224)) / 255.0\n",
    "        image_array.append(im)\n",
    "        label_array.append(label)\n",
    "        \n",
    "    label_array = encode_label(label_array)\n",
    "    return np.array(image_array), np.array(label_array)\n",
    "batch_size = 16\n",
    "epoch_shuffle = True\n",
    "weight_classes = True\n",
    "epochs = 15\n",
    "\n",
    "# Split train data up into 80% and 20% validation\n",
    "train, validate = np.split(df.sample(frac=1), [int(.8*len(df))])\n",
    "print(\"Training on:\", len(train), \"samples\")\n",
    "print(\"Validation on:\", len(validate), \"samples\")\n",
    "\n",
    "    \n",
    "for e in range(epochs):\n",
    "    print(\"Epoch: \", str(e+1) + \"/\" + str(epochs))\n",
    "    if epoch_shuffle:\n",
    "        train = train.sample(frac = 1)\n",
    "    for it in range(int(np.ceil(len(train)/batch_size))):\n",
    "\n",
    "        X_train, y_train = get_batch(train, it*batch_size, batch_size)\n",
    "\n",
    "        model.train_on_batch(X_train, y_train)\n",
    "        \n",
    "\n",
    "model.save(\"Model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test on training set\n",
    "batch_size = 16\n",
    "\n",
    "errors = 0\n",
    "good_preds = []\n",
    "bad_preds = []\n",
    "\n",
    "for it in range(int(np.ceil(len(validate)/batch_size))):\n",
    "\n",
    "    X_train, y_train = get_batch(validate, it*batch_size, batch_size)\n",
    "\n",
    "    result = model.predict(X_train)\n",
    "    cla = np.argmax(result, axis=1)\n",
    "    for idx, res in enumerate(result):\n",
    "        print(\"Class:\", cla[idx], \"- Confidence:\", np.round(res[cla[idx]],2), \"- GT:\", y_train[idx])\n",
    "        if cla[idx] != y_train[idx]:\n",
    "            errors = errors + 1\n",
    "            bad_preds.append([batch_size*it + idx, cla[idx], res[cla[idx]]])\n",
    "        else:\n",
    "            good_preds.append([batch_size*it + idx, cla[idx], res[cla[idx]]])\n",
    "\n",
    "print(\"Errors: \", errors, \"Acc:\", np.round(100*(len(validate)-errors)/len(validate),2))\n",
    "\n",
    "#Good predictions\n",
    "good_preds = np.array(good_preds)\n",
    "good_preds = np.array(sorted(good_preds, key = lambda x: x[2], reverse=True))\n",
    "\n",
    "fig=plt.figure(figsize=(16, 16))\n",
    "for i in range(1,6):\n",
    "    n = int(good_preds[i,0])\n",
    "    img, lbl = get_image_from_number(n, validate)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    fig.add_subplot(1, 5, i)\n",
    "    plt.imshow(img)\n",
    "    lbl2 = np.array(int(good_preds[i,1])).reshape(1,1)\n",
    "    sample_cnt = list(df.landmark_id).count(lbl)\n",
    "    plt.title(\"Label: \" + str(lbl) + \"\\nClassified as: \" + str(decode_label(lbl2)) + \"\\nSamples in class \" + str(lbl) + \": \" + str(sample_cnt))\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ]
}